"ID","Type","Name","Documentation","Specialization"
"id-4b9cc51f183f45c4830114a6b42a6cd6","ArchimateModel","EDC-IDQ-Axon","",""
"id-061ca94c29be4d82bb50078d9c7eaf3b","BusinessActor","Biz End  Users","Biz End Users and Data Stewards will primarily use Axon
Technologists will primarily use the Enterprise Data Catalog
",""
"id-34443813757b44e9bcec2fb2263cd018","BusinessActor","Data Stewards","Biz End Users and Data Stewards will primarily use Axon
Technologists will primarily use the Enterprise Data Catalog",""
"id-a8f4316d226b466bb3c7f1b30631dce8","BusinessActor","Technologists","Technologists will primarily use the Enterprise Data Catalog
Biz End Users and Data Stewards will primarily use Axon",""
"id-d8d8439bdf8b4331b38a7118660f1ce7","BusinessService","Business Service","",""
"id-e960173a3a1d487aad0f4bc591b90672","BusinessService","Business Service","",""
"id-97e72bd9e96d415abde9d5f3ae04aba8","BusinessService","Data Analysis / Data Profiling","This is a manual process where Technologists, Biz End Users, and Data Stewards transform ""raw data"" into ""usable information"".

The ""usable information"" will be published in formal reports in Tableau, PowerBI, and other reporting tier components.",""
"id-1346f31862034439a6fc59906fc1dffd","ApplicationComponent","All CNO Source  systems","... represents any and all CNO source system whose data is pulled into the CNO Enterprise Data Warehouse.

These ~500 CNO systems are defined in the Enterprise System Catalog:   http://tableau.cnoinc.com/views/EnterpriseSystemCatalog/Applications?:iid=1%22&:isGuestRedirectFromVizportal=y&:embed=y 

EDC/IDQ/Axon are focused on tracking Personal-Data to comply with CCPA/CPRA, the current location of ALL Personal-Data need to be tracked.

Therefore ""All CNO Source systems"" includes both operational (day to day) source systems as well as data stores within the EDW.",""
"id-11b04caacb784e2fb83cb6d7e9da51f8","ApplicationComponent","Application Component","",""
"id-cb83200637304df9bade2fc2c3875aa2","ApplicationComponent","Axon","Axon integrates and makes available all information provided by EDC and IDQ

Policies - are attached to each Axon data-set that specifies the Terms of Use

Axon business glossary has 800 - 900 elements.

",""
"id-0318c251fd664fa88f0d37e3e5161108","ApplicationComponent","Enterprise Data Catalog - EDC","Enterprise Data Catalog (EDC) primarily 
 - identifies the types or catagories of data, and
 - data lineage

Access Management:

Authentication is SAML based using Microsoft AD
Authorization is Role based and is setup using ""Security"" console on EDC UI (https://edc-admin-console-uat.cnoinc.com/administrator/)

User activity is logged under ""Audit"" console: https://edc-admin-console-uat.cnoinc.com/administrator/#log_UserActivity/ (Under Administration -> User Activity)

Application logs: Application logs can be found under ""Logs"" console under ""Administrator"" (https://edc-admin-console-uat.cnoinc.com/administrator/) There are tabs for ""Domain"" and ""Service""and ""Log Aggregator"".

Node/Service statistics can be viewed under https://edc-admin-console-uat.cnoinc.com/administrator/#admin_Domain. There are tabs for Domain, Services and Nodes, Connections and Schedules. ""Services and Nodes"" console shows details on various services and details such as properties, Resources, Permissions, Diagnostics, Plugs and Monitoring Configuration.
Monitor console provides real time and historical statistics at Summary and Execution levels (https://edc-admin-console-uat.cnoinc.com/administrator/#monitoring) - Ad Hoc Jobs, Deployed Mappings and Workflows, Requests and Connections and Resource usage. Resource usage shows graphs for CPU usage and Memory usage.
",""
"id-cab99a7a2333447ca8dbc1c76fa957e9","ApplicationComponent","Informatica Data Quality - IDQ","Informatic Data Quality (IDQ) primarily focuses upon:
 - DATA PROFILING: which provides statistical information of 
   each data element VALUES, and
 - EXCEPTION MANAGEMENT: which identifies bad or suspiciously bad 
   data that will require DATA CLEANSING
",""
"id-97fdc45a3b8a4e239d2444c462083cc6","ApplicationEvent","Data Discovery Scans","",""
"id-40c42527b35a4985807953f3d2526e66","ApplicationEvent","Data Transfer","",""
"id-17803b0031954996a27c7026b755c4be","ApplicationFunction","Application Function 1","",""
"id-843ff24c40754dc8b274e543a4c9914e","ApplicationFunction","Application Function 2","",""
"id-ca0eb3a32cb648daa4d2af077c567a06","ApplicationFunction","Ask a question to data steward/owner","Allows Q&A to ask a question and receive responses.",""
"id-d379f600687d483a855775499ab53f0d","ApplicationFunction","Basic Metadata","This is primarily table names, column names and declarative foreign keys.",""
"id-035492aa093c4dc88ad59df78255829a","ApplicationFunction","Data Lineage","The Data Lineage is captured for each data element.
Data Lineage is:  
(1) what Source System first captured the data, as well as 
(2) any & all of the downstream applications and data warehouse components where the data element resides",""
"id-92727d358dad4a0da1c3ddfe4789b7c2","ApplicationFunction","Data Profiling","provides statistical information of each data element VALUES",""
"id-8b960a02da574c1b89a204cc59c4abac","ApplicationFunction","Exception Management","identifies bad or suspiciously bad data that will require Data Cleansing",""
"id-1c8267b4a0654d389952312bb28dd2af","ApplicationFunction","Identify Data Elements by Type","For example, any data element that fits the patterns 99999, 9999-9999, or 999999999 is a ZipCode",""
"id-8b7a3d06007c4829a4419ce492f1fcb4","ApplicationFunction","Identify PII and PHI","Identifies data elements that require a high degree of protection:
 - PII = personally identifyable information
 - PHI = protected health information

Includes the MOVEMENT of sensitive data; this is essential when we are required to DELETE data as required by the CCPA/CPRA (California Consumer Privacy Act / California Privacy Rights Act )",""
"id-e6ade059a9aa4a8d9c226b790626a141","ApplicationFunction","IDQ Orphaned Foreign keys ","Referential Integrity Analysis: if orphaned records are present, it denotes a violation of foreign key relationship (a corresponding record is not present in parent table).",""
"id-8e773ca1f6314e78b5c1c6ad1d6294ed","ApplicationFunction","Infer Relationships","When source systems do not specify primary or foreign key relationships explicitly, they  are inferred by comparing the data values of columns from multiple tables",""
"id-a5edac1ff0c84632bcb0e80369d8163b","ApplicationFunction","Manage Business Glossary","",""
"id-7b5ed25a82e84b19b8c5499d73814c68","ApplicationFunction","Natural Language Processor","To Do:  needs description & connection to _______",""
"id-bb673a233de74716b38335be34910bc9","ApplicationService","","",""
"id-b29897bb8d054b0198b356df00b6fd06","ApplicationService","Admin Console UI-8443","UAT:https://LXINFEDCAPM01.conseco.ad:8443
PROD: https://LXINFEDCAPP01.conseco.ad:8443

CONNECTIONS:
 - navigation: home page --> Manage --> Connections
 - definition: a Connection is an application (usually a Source System) that will be Scanned; 
 - Scans: perform Data Discovery
 - The CNO contractual agreement allows 600.
 - Connection Naming Convention _____________

LOGS:
 - navigation: home page --> Logs
 - definition: a Log records status and error information as Scans are processed

SECURITY:
 - navigation: home page --> Security
 - definition: this set of web pages shows Users, Groups, Roles, and
    RBAC (role based access control): Membership, Privileges,  and Permissions 

SERVICES & Nodes:
 - navigation: home page --> Manage --> Domains
 - navigation: home page --> Manage --> Services & Nodes
 - definition: a Service is a process that provides various functionality within the EDC ecosystem
",""
"id-262b292b6d0446bfa000ab88e6313d40","ApplicationService","Document and manage business glossary","",""
"id-85600da0e457409caca513cd1e4d6b1c","ApplicationService","LDM Admin-9085","UAT: https://lxinfedcapm01.conseco.ad:9085/ldmadmin
PROD: https://edc-catalogadmin.cnoinc.com:9085/ldmadmin

LDM = Live Data Map

DEFINITIONS:
 - Asset is 
 - Connection is an application (usually a Source System) that will be Scanned. 
    The CNO contractual agreement allows 600.
    Connection Naming Convention _____________
 -  Resource are mostly Connections along with _____
 -  Task is a Scan job that has been run at a specific date and time (?)
 - Domain is an algorthim that discerns the data type of each data element within a Connection
    Examples of data types can be names, city, zip code
    Many algorithms are implemented as Regular Expressions

OVERVIEW tab: Shows Assets, Resources, and Task statuses

LIBRARY tab:  Shows Domains

MONITORING tab: Shows all Tasks that have been run.",""
"id-0bef37685c9f474ba7410e9a289a7d87","ApplicationService","LDM Catalog-9085","UAT:   https://lxinfedcapm01.conseco.ad:9085/ldmcatalog
PROD:  https://edc-catalog.cnoinc.com:9085/ldmcatalog

Home Page: 
Displays 3 navigation items: Discovery tab, Analytics tab, Resources link

DISCOVERY tab -- displays the RESOURCES link, which displays our Resources or Connections.  
  - The CNO contractual agreement allows 600 Resources/Connections.
  - .Connection Naming Convention _____________
Click on a Resource/Connection to provide detailed information regarding, such as Tables, Columns, and Relationships

ANALYTICS tab - Dashboard:
 User Adoption tab:
 - displays metrics regarding aggregate usage
Data Asset Inventory tab:
 - displays information regarding our Resources/Connection
Data Asset Enrichment tab:
 - displays aggregate information regarding Business Terms, Domains, and Data Lineage
",""
"id-fc07620d234845b8800c6ca1ad7ce1e4","ApplicationService","Model Repository Service","",""
"id-4e8836f74c86420dad287e38a1b132dd","ApplicationService","Search and Understand data, it's lineage and relationships","",""
"id-b0e5010f2af241bcb5dc07ef89c464bd","ApplicationService","Search and Understand data, it's lineage and relationships","UAT:   https://lxinfedcapm01.conseco.ad:9085/ldmcatalog
PROD:  https://edc-catalog.cnoinc.com:9085/ldmcatalog

Home Page: 
Displays 3 navigation items: Discovery tab, Analytics tab, Resources link

DISCOVERY tab -- displays the RESOURCES link, which displays our Resources or Connections.  
  - The CNO contractual agreement allows 600 Resources/Connections.
  - .Connection Naming Convention _____________
Click on a Resource/Connection to provide detailed information regarding, such as Tables, Columns, and Relationships

ANALYTICS tab - Dashboard:
 User Adoption tab:
 - displays metrics regarding aggregate usage
Data Asset Inventory tab:
 - displays information regarding our Resources/Connection
Data Asset Enrichment tab:
 - displays aggregate information regarding Business Terms, Domains, and Data Lineage
",""
"id-f721b29a47d14539b00a4fc0ef887fbb","ApplicationService","UI Services","",""
"id-a51d03a6710b416ebd6f2cf9c94c57ef","ApplicationService","UI-9443","UAT:
     https://lxinfaxonapm01.conseco.ad:9443/
     ToDo: create link to ""CNO_Informatica_Axon_User_Access_Guide_UAT.docx""

PROD:
     https://axon.cnoinc.com:9443/ or
     https://lxinfaxonapp01.conseco.ad:9443/
   ToDo: create link to ""CNO_Informatica_Axon_User_Access_Guide_PROD.pdf""

Authentication and Authorization: Authentication is done using CNO AD. Authorization is done using LDAP setup (ldapcml.conseco.ad) within Axon (https://axon.cnoinc.com:9443/admin/customizeconfig - LDAP Server). Roles are SuperAdmin, Admin, Webuser (Edit), Webuser (View only).

Application Logs can be found under: https://axon.cnoinc.com:9443/admin/logs and https://axon.cnoinc.com:9443/admin/audit/view",""
"id-11142631d15a407f8529a8f6bd446dd6","CommunicationNetwork","Communication Network","
work in progress",""
"id-cff2539dbcf54412bfe4a72e0075506b","Node","AXON NODE01 - LXINFAXONAPP01 (E8a V4) - Azure","Purpose: 
Axon Overview Meeting 2-Apr-2021 -- Brian Bussing,  <bbussing@informatica.com> 
https://informatica.zoom.us/rec/share/lDJpiSPn7Q7zBj1kKNcC5ppF4MnVJYQI4QHnmOEwBx68M22bVPlSAg5oi0x80EXP.kNwK-KRUaOBVQFgE
Access Passcode: #CNOInfa2021!

Version: SQL Server 2019

CNO is still single region and multi-region will be explored",""
"id-90b698494708433d89ff6ecb4a435e71","Node","Azure SQL DB","",""
"id-49873092cd1041c2b1e3617d267966c0","Node","Azure Sql DB","Purpose: 

",""
"id-9cbdcd45854d4481ad1e4d62224f5432","Node","Azure SQL DB","Purpose: ______________

We have only one environment: PROD; no UAT or DEV -- correct ?

Azure SQL DB  (PaaS)  (EDC)

OS version need to be checked with INFA to RHEL 7.9",""
"id-1a653d3b87934d70adddbb4fd12a1a93","Node","Azure SQL DB","Purpose: 

Version: SQL Server 2019

CNO is still single region and multi-region will be explored",""
"id-40acd5c869c542408fdf5ad7a4b9f672","Node","Azure SQL DB (Active)","",""
"id-4335f1a7baf74d01aedb2af95cdccc5c","Node","Azure SQL DB (Active)","",""
"id-c1d51bb747a84191aee1749dcd35595d","Node","Azure SQL DB (Active) (copy)","",""
"id-7a546b567f314ed1830e04b14665ec9b","Node","Azure SQL DB (Passive)","exists only in Production Environment",""
"id-aaac5e85b1014e998c58b2c2fab733c1","Node","Azure SQL DB (Passive) (copy)","exists only in Production Environment",""
"id-a59a08d82b7048d08e01f3a7538174c1","Node","Azure SQL DB (Passive) (copy)","exists only in Production Environment",""
"id-f6d709faa3754b4abcd68e81f21ce693","Node","EDC NODE01 - LXINFEDCAPP01 (D32a V4)","Purpose: The Administrator tool (Informatica Administrator) is the administration tool that you use to administer the Informatica domain and security. The Administrator tool is a thin or web client application.

OS version need to be checked with INFA to RHEL 7.9

For DEV: 
   CNO recommends scale down for initial phase and expand to 32 core as needed.
",""
"id-e848b36f924747909bf472a6b70466f4","Node","EDC NODE02 - LXINFEDCAPP02 (D32a V4)","Purpose: The Administrator tool (Informatica Administrator) is the administration tool that you use to administer the Informatica domain and security. The Administrator tool is a thin or web client application.

OS version need to be checked with INFA to RHEL 7.9

For DEV: 
   CNO recommends scale down for initial phase and expand to 32 core as needed.",""
"id-afabcf02281d49fe904f22156a06962a","Node","EDC Services - (Azure) Region - EAST US 2","",""
"id-0b88456a80ce489ebe4d4a798529f67d","Node","Hadoop Cluster Services","Hadoop Cluster services
 

EDC relies on Hadoop technologies to achieve scalability, performance and flexibility of an enterprise class application that will hold millions of information and serve thousands of requests at the same time as user community will grow in your company. To that end we support different deployment types

using an existing cluster if your organisation is familiar with the technology, it is the best option.
using an Informatica managed cluster, you just have to provision required hardware and the Informatica domain will take care of deploying the cluster for you.
 

The Hadoop services that are leveraged by the catalog service are as follow:

Zookeeper: Maintains the configuration of the Hadoop cluster services across all the cluster nodes
HDFS : EDC stores the catalog information in HDFS , to store the catalog contents a special HDFS directory is created. /Informatica/LDM/<ServiceClusterName>. The directory needs to be owned by serviceuser name ( service principal In case of Kerberos)
Yarn: Stands for Yet Another Resource Negotiator, YARN uses a cluster wide resourcemanager and per node nodemanager daemons to manage Hadoop resource distribution and usage.When catalog service is started , it starts three applications in YARN
HBase: Manage and provide non SQL document storage capabilities on top of HDFS with scalability. This is where all the sources metadata will be stored.
SolR: Provide indexing and search capabilities, EDC build indexes of the metadata stored in HBase and allow fuzzy search and fast retrieve of the metadata
Spark: Ingest service used to collect, transform data and update HBase store and SolR indexes
Scanners: applications launched as batch job to extract metadata, run profiling and data discovery tasks and move the results over to the Catalog queue to be process by the ingestion service. The scanner application will pushdown the execution of the profiling and data discovery to the source application whenever possible (for databases, hadoop clusters, etc.)",""
"id-263042b48c3449ee898c0a7c8e35363c","Node","Hadoop Node01 & Node02 (Availability Set)","Embedded Hadoop Cluster - HortonWorks",""
"id-4d3bc1310202466c84cbfe32717063cf","Node","Hadoop Node01 - LXINFEDCHCP01 - E8a v4","Purpose: 

OS version need to be checked with INFA to RHEL 7.9

1. Hbase - Stores the catalog and audit information

2. Solr - Enables the search on catalog

3. Spark - Enables ingestion of everything that goes into catalog.",""
"id-8059f790d59f45dfaaff86d1438ac63f","Node","Hadoop Node02 - LXINFEDCHCP02 - E8a V4","Purpose: 

OS version need to be checked with INFA to RHEL 7.9",""
"id-4b4026e8a8754fe389ce37ae753ee713","Node","Hadoop Node03","exists only in Production and User Acceptance Test Environments",""
"id-3e5474c1e2a04bdf9cd17ee7b287caba","Node","Hadoop Node3 - LXINFEDCHCP03 - E8a V4","Purpose: 

OS version need to be checked with INFA to RHEL 7.9",""
"id-c99a49b8a83b4806b4821ad963f9ee49","Node","IDQ Node 2 - LXINFDQP05","Is ""IDQ Node02"" the correct name?",""
"id-026ba9c10b644a06995d36e2c3f7146c","Node","IDQ Node01","Is ""IDQ Node01"" the correct name?",""
"id-7a04d6f75f1b4a458b7a10ab58cbf817","Node","IDQ Node02","Is ""IDQ Node01"" the correct name?",""
"id-1f2cd47f8c2e4532baeb758b85a81b93","Node","IDQ Node1 - LXINFDQP04","Is ""IDQ Node01"" the correct name?",""
"id-8eb4a58329824cfdbb8968dd64d81f99","Node","IDQ Services - (on prem)","",""
"id-8a9e66ccba4d47088f3c99a48f6ad996","Node","Linux Server Specs","
https://teams.microsoft.com/_#/files/General?threadId=19%3A95cee131907b43589037d71617073bc2%40thread.tacv2&ctx=channel&context=LinuxServerSpecs&rootfolder=%252Fsites%252FITDataManagementandDelivery%252FShared%2520Documents%252FGeneral%252FData%2520Management%2520EDC-IDQ-Axon%2520BigID%252FLinuxServerSpecs

",""
"id-109a5f27bfdb4d899d68bfde50e641f3","Node","Node/Host","",""
"id-39d3470a0a3d4f87b9354f76a7a89a68","TechnologyCollaboration","Availability Zone 1","Load Balancer does not exist in Development Environment",""
"id-4a06d6db0da3452aaa1533f9949ab025","TechnologyCollaboration","Availability Zone 1","exists only in Production Environment",""
"id-ebffdcaf8ac04bcd91f28f95ad2ee438","TechnologyCollaboration","Availability Zone 2","exists only in Production Environment",""
"id-cbc33dec5b68455ab5ec24c2a73a3fcb","TechnologyCollaboration","Availability Zone 2","",""
"id-65e316020b694ca39a2321f72075b417","TechnologyCollaboration","Azure Cloud - Virtual Network","",""
"id-03684da37f7d4b7db343ff934d803324","TechnologyCollaboration","High Availability (copy)","exists only in Production Environment",""
"id-c615ebcc6abd4687ab48d6c668de4de8","TechnologyCollaboration"," High-Availabiilty: F5 ASM WAF","High-Availabiilty F5 ASM WAF and EDC Node02  do not exist in Development Environment

https://devcentral.f5.com/s/articles/the-big-ip-application-security-manager-part-1-what-is-the-asm 

The BIG-IP Application Security Manager (ASM)  is a Layer 7 ICSA-certified Web Application Firewall (WAF) that provides application security in traditional, virtual, and private cloud environments.",""
"id-c8e9b418b73c49d281e158903c6c87cc","TechnologyCollaboration"," High-Availabiilty: F5 ASM WAF","High-Availabiilty F5 ASM WAF and IDQ Node02  do not exist in Development Environment

https://devcentral.f5.com/s/articles/the-big-ip-application-security-manager-part-1-what-is-the-asm 

The BIG-IP Application Security Manager (ASM)  is a Layer 7 ICSA-certified Web Application Firewall (WAF) that provides application security in traditional, virtual, and private cloud environments.",""
"id-a93020af920842db8f5eb9a5d38fdb46","TechnologyCollaboration","Technology Collaboration","",""
"id-9bd2e6ecf7ca42afba93066a27830c34","TechnologyCollaboration","Technology Collaboration","",""
"id-a2305b9ea2f840c5837f301cdaf2f4f0","TechnologyFunction","Technology Function 1","",""
"id-2f2331cfa27e49f5a63fe2d81e137f1f","TechnologyFunction","Technology Function2","",""
"id-deeb631bc15544339d87e0cb2546b06b","TechnologyService","AdminConsole UI-8443","The Administrator tool (Informatica Administrator) is the administration tool that you use to administer the Informatica domain and security. The Administrator tool is a thin or web client application.",""
"id-09984a192a9c4988a5bb984d1edfdd5d","TechnologyService","AdminConsole UI-8443 (copy)","",""
"id-c6328c76585a4e3a881c4cf52386f224","TechnologyService","Ambari UI-8080/8443","Ambari is a web based tool for provisioning, managing, and monitoring Apache Hadoop clusters    It supports HDFS, MapReduce, Hive, HCatalog, HBASE, ZooKeeper, Oozie, Pig, and Sqoop services.

Used to start/stop Informatica cluster service using REST API.
Cluster service monitor the health of the Hadoop services using Ambari REST API.
Ambari provides status of the services via the Ambari Metrics service.",""
"id-d43c0d43536f405c9ddf564baf638595","TechnologyService","Ambari UI-8080/8443 (copy)","",""
"id-93acbe0a4d2943a6a80a4e0fd2ef656b","TechnologyService","Availability Zone 1","",""
"id-e96475a893b04ad99e46ca159d5fb912","TechnologyService","Availability Zone 2","",""
"id-88a4cbd05c5f4252852c7b147387385b","TechnologyService","AXON UI - 9443/9999","These two end points are connected via the Load Balancer
https://lxinfedcapp01.conseco.ad:9085/ldmadmin (Primary- EDC Node01)
https://lxinfedcapp02.conseco.ad:9085/ldmadmin (Secondary- EDC Node02)",""
"id-291dbde0884c4126a73b7f9a67aaa423","TechnologyService","Azure Cloud - Virtual Network","",""
"id-1d1aaf9d0f844b7a9efb443638ccba62","TechnologyService","Azure Load Balancer","",""
"id-6debbaf9151d4b2fa7700f745941aadb","TechnologyService","Azure Load Balancer (copy)","",""
"id-e6e8afb710a241c0b117f31e1239952e","TechnologyService","Azure Load Balancer (copy) (copy)","",""
"id-9a416aff655b492799e023628c7c6650","TechnologyService","Catalog Adminconsole UI-9485","Purpose: Use Catalog Administrator to 
    consolidate the administrative tasks for resources, attributes, and schedules. 
    You launch 
        - Catalog Administrator 
from Informatica Administrator to 
            create resources and 
            run the scans and linkage between resources. 

Catalog Service is needed to run the Enterprise data Catalog application and manage the connections between the Enterprise data Catalog components. You can configure the general, application service, and security properties of the Catalog Service",""
"id-760a4067ef584f61b0d24ca3809e945d","TechnologyService","Catalog Adminconsole UI-9485 (copy)","Purpose: Use Catalog Administrator to consolidate the administrative tasks for resources, attributes, and schedules. You launch Catalog Administrator from Informatica Administrator to create resources and run the scans and linkage between resources. ",""
"id-2277ab1f63084977867a1725d62842ba","TechnologyService","Catalog UI & Rest-9485","Purpose: The catalog represents an indexed inventory of all the data assets in the enterprise that you configure in Catalog Administrator. Enterprise Data Catalog organizes all the enterprise metadata in the catalog and enables the users of external applications discover and understand the data.The catalog stores all the metadata extracted from external data sources. You can find metadata and statistical information, such as profile results, data domains, and data asset relationships, from the catalog.

The catalog service coordinates user interaction with catalog metadata stored on the Hadoop cluster. It is the bridge that connects Informatica domain to catalog cluster.",""
"id-447f392762694cdba0505500cac2b748","TechnologyService","Catalog UI & Rest-9485 (copy)","Purpose: The catalog represents an indexed inventory of all the data assets in the enterprise that you configure in Catalog Administrator. Enterprise Data Catalog organizes all the enterprise metadata in the catalog and enables the users of external applications discover and understand the data.The catalog stores all the metadata extracted from external data sources. You can find metadata and statistical information, such as profile results, data domains, and data asset relationships, from the catalog.",""
"id-eeef0e7bc82142aabb67ccca625f59e5","TechnologyService","Content Management Service-8505","Purpose: The Content Management Service is an application service that 
     manages reference data. 
    It provides reference data information 
   to the Data Integration Service and to the 
   Developer and Analyst tools.

Content Management Service is an application service that manages reference data. A reference data object contains a set of data values that you can search while performing data quality operations on source data. The Content Management Service also compiles rule specifications into mapplets. Content Management Service uses the Data Integration Service to run mappings to transfer data between reference tables and external data sources. The Content Management Service also provides transformations, mapping specifications, and rule specifications with the following types of reference data:
Address reference data
Identity populations
Probabilistic models and classifier",""
"id-0f5ac1631ea843e58bba0a139c3b8ab8","TechnologyService","Content Management Service-8505 (copy)","Purpose: The Content Management Service is an application service that manages reference data. It provides reference data information to the Data Integration Service and to the Developer and Analyst tools.",""
"id-cb7a649883be44cd9fd34606146cd652","TechnologyService","Data Integration Service-8495","Purpose: The Data Integration Service is an application service in the Informatica domain that 
    performs data integration tasks for 
    Informatica Analyst and Informatica Developer.

In EDC whenever an user run scans on resources and view the metadata and profiling statistics in Enterprise Data Catalog, the client tool sends requests to the Data Integration Service to perform the enterprise data discovery.
The Data Integration Service execute data profiling and discovery.
Is used during data semantic discovery and basic profiling of data assets (% of Nulls, distinct values etc.)
",""
"id-d09a5bcd509e4c61950e9378b520c1f7","TechnologyService","Data Integration Service-8495 (copy)","Purpose: The Data Integration Service is an application service in the Informatica domain that performs data integration tasks for Informatica Analyst and Informatica Developer.The Data Integration Service is an application service in the Informatica domain that performs data integration tasks for Informatica Developer.",""
"id-0a7f8bf271474a679d919ab018d868a5","TechnologyService","Domain Core Service-6005","Purpose: TBD",""
"id-9f8680122b954306ba3e3a0a1071b60f","TechnologyService","Domain Core Service-6005 (copy)","Purpose: TBD",""
"id-636f1fbd8e64438d895e9c857714fd80","TechnologyService","HDFS DataNode - 50075/50475","Actual  file contents are stored in distributed Data Nodes.   The contents are split across multiple Data Node to take advantage of parallel processing which improves performance.

Catalog information is stored in HDFS.",""
"id-0627f8aaa6a54940a03c88afebcd7643","TechnologyService","HDFS DataNode - 50075/50475 (copy)","",""
"id-fc6bfdc27c074142aaff97e861e6356c","TechnologyService","Informatica Hadoop Service-9475","Purpose: connection to Hadoop Cluster Services (?)",""
"id-3460cb6dc31947ec838bb58363d25c02","TechnologyService","Informatica Hadoop Service-9475 (copy)","Purpose: connection to Hadoop Cluster Services (?)",""
"id-8f505c561f794ca6be105277360921a8","TechnologyService","Job History UI - 19888","
https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html

Job History allows the user to get status on finished applications.
",""
"id-62c30ddafa2f4010b0c123c0c8718420","TechnologyService","Job History UI - 19888 (copy)","",""
"id-e4950b32807c41b7b7e9c60e6be427c9","TechnologyService","LDM Admin-9085","These two end points are connected via the Load Balancer
https://lxinfedcapp01.conseco.ad:9085/ldmadmin (Primary- EDC Node01)
https://lxinfedcapp02.conseco.ad:9085/ldmadmin (Secondary- EDC Node02)",""
"id-d6b39fab02454737a3af86162233932a","TechnologyService","LDM Catalog-9085","These two end points are connected via the Load Balancer
https://lxinfedcapp01.conseco.ad:9085/ldmcatalog (Primary- EDC Node01) 
https://lxinfedcapp02.conseco.ad:9085/ldmcatalog (Secondary - EDC Node02)",""
"id-fb4b558a2827484ba682df5b295dc934","TechnologyService","Model Repository Service","Purpose: The Model Repository Service is an application service that manages model repository that stores metadata created by the Enterprise Data Catalog. MRS stores the underlying contents in a relational database to enable collaboration among the tools and services. Model repository primarily stores the resource configuration and data domain information. When you access a Model repository object from the Enterprise Data Catalog tools or the Data Integration Service, the client or service sends a request to the Model Repository Service. The Model Repository Service process fetches, inserts, and updates the metadata in the Model repository database tables.",""
"id-af46fea7a5a04571bc6c51365917ac14","TechnologyService","Model Repository Service database (MRS)","Purpose: The Model Repository Service is an application service that manages model repository that stores metadata created by the Enterprise Data Catalog. MRS stores the underlying contents in a relational database to enable collaboration among the tools and services. Model repository primarily stores the resource configuration and data domain information. When you access a Model repository object from the Enterprise Data Catalog tools or the Data Integration Service, the client or service sends a request to the Model Repository Service. The Model Repository Service process fetches, inserts, and updates the metadata in the Model repository database tables.

MRS stores all the configuration and connectivity information for EDC resources.",""
"id-93b4513806d640a0aa0b186aecb3607c","TechnologyService","NameNode UI-500070/50470 (copy)","",""
"id-5bc77c713534493cb2cfa1a2cae3a864","TechnologyService","NameNode UI-50070/50470","NameNode Is a daemon that track meta data
• File Name
• Permissions
• Ownership
• File Location
 for each file in HDFS (Hadoop distributed file system).  Actual  file contents are stored in Data Nodes.
",""
"id-a5be97498e6a4b7f8aab76c59112bc9c","TechnologyService","NodeManager UI - 8044","Node Manager manages, allocates, and monitors resource (cpu, ram, disk, ... and oversees the containers running MapReduce processes on a single node/host within a Hadoop cluster.",""
"id-53dfeecbc4304fd39fcb4cc036a8c380","TechnologyService","NodeManager UI - 8044 (copy)","",""
"id-8f2682fbc95e414bafb3c7a6ce4def7e","TechnologyService","Profile warehouse database (PWH)","Purpose: Profile warehouse database (PWH) is used by the Data Integration services to store the profile statistics and value frequency are temporarily stored before being moved over to the HBase database by the Spark ingestion services.",""
"id-4dab94584d064dcdb535664909e3de75","TechnologyService","Reference database (REF)","Purpose: Reference database  (REF) is used by the Content Management services to store reference data used for data discovery such as names, synonyms, location reference data, etc.",""
"id-6db6c65de4a44e44b7726112c054c21b","TechnologyService","rg-ccpa-prod-eu2","",""
"id-51c52574303d45b38e7e042bf0ed06b0","TechnologyService","UI Web App","",""
"id-ee5bbc6fbddd42a9b8712784a5981665","TechnologyService","Yarn Resource Manager UI -8088/8090","Yarn is a resource manager that facilitates MapReduce processes.  MapReduce is a software engineering pattern that splits VERY large jobs into segments that are run in parallel, thereby improving performance.

Spark, HBase and Solr apps run on YARN.",""
"id-9e31be54abed430ca81da4d9c6df265c","TechnologyService","Yarn RM UI -8088/8090 (copy)","",""
"id-6848a9bf16a046c9825a931220fdd8c4","ImplementationEvent","Implementation Event","",""
"id-9399c6dbd8fd4a2992dec19dbfd5b1c4","WorkPackage","Resource group","",""
"id-26761c9b35bc411ba83560d1c8d84a6c","WorkPackage","Work Package","",""